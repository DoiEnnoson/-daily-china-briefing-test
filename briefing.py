import email
import feedparser
import imaplib
import json
import os
import re
import requests
import smtplib
import time
import warnings
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
from collections import defaultdict
from datetime import date, datetime, timedelta
from email.mime.text import MIMEText
from email.utils import parsedate_to_datetime

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

# Pfad zu den Holiday JSON Dateien und CPR-Cache (relativ zum Script-Verzeichnis)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHINA_HOLIDAY_FILE = os.path.join(BASE_DIR, "holiday_cache", "china.json")
HK_HOLIDAY_FILE = os.path.join(BASE_DIR, "holiday_cache", "hk.json")
CPR_CACHE_FILE = os.path.join(BASE_DIR, "cpr_cache.json")

def load_holidays(filepath):
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
            return set(item["date"] for item in data.get("holidays", []))
    except Exception as e:
        print(f"Fehler beim Laden der Feiertage aus {filepath}: {e}")
        return set()

def is_holiday(today_str, holidays_set):
    return today_str in holidays_set

def is_weekend():
    return date.today().weekday() >= 5

# Neue Funktion: CPR-Cache laden
def load_cpr_cache():
    try:
        if os.path.exists(CPR_CACHE_FILE):
            with open(CPR_CACHE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        return {}
    except Exception as e:
        print(f"Fehler beim Laden des CPR-Cache: {e}")
        return {}

# Neue Funktion: CPR-Cache speichern
def save_cpr_cache(cache):
    try:
        with open(CPR_CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"Fehler beim Speichern des CPR-Cache: {e}")

# Werte vorladen (global)
today_str = date.today().isoformat()
china_holidays = load_holidays(CHINA_HOLIDAY_FILE)
hk_holidays = load_holidays(HK_HOLIDAY_FILE)
is_holiday_china = is_holiday(today_str, china_holidays)
is_holiday_hk = is_holiday(today_str, hk_holidays)
is_weekend_day = is_weekend()

# === üß† Wirtschaftskalendar (Dummy) ===
def fetch_china_economic_events():
    return [
        "‚Ä¢ 03.06. (Di) 03:45 ‚Äì Caixin Manufacturing PMI (Mai) | Prognose: 50.6 | Vorher: 50.4",
        "‚Ä¢ 05.06. (Do) 03:45 ‚Äì Caixin Services PMI (Mai) | Prognose: 51.1 | Vorher: 50.7",
        "‚Ä¢ 05.06. (Do) 03:45 ‚Äì Caixin Composite PMI (Mai) | Prognose: 50.7 | Vorher: 51.1",
        "‚Ä¢ 07.06. (Sa) 10:00 ‚Äì Foreign Exchange Reserves (Mai) | Prognose: $3.35T | Vorher: $3.282T"
    ]

# === üîê Konfiguration aus ENV-Variable ===
config = os.getenv("CONFIG")
if not config:
    raise ValueError("CONFIG environment variable not found!")
pairs = config.split(";")
config_dict = dict(pair.split("=", 1) for pair in pairs)

# === Google Mapping ===
source_categories = {
    "Wall Street Journal": "EN",
    "Financial Times": "EN",
    "Reuters": "EN",
    "The Guardian": "EN",
    "New York Times": "EN",
    "Bloomberg": "EN",
    "Politico": "EN",
    "FAZ": "DE",
    "Welt": "DE",
    "Tagesspiegel": "DE",
    "NZZ": "DE",
    "Finanzmarktwelt": "DE",
    "Der Standard": "DE",
    "Frankfurter Rundschau": "DE",
    "Le Monde": "FR",
    "Les Echos": "FR",
    "Le Figaro": "FR",
    "SCMP": "ASIA",
    "Nikkei Asia": "ASIA",
    "Yicai": "ASIA"
}

# === Google-News: Feed-Definition ===
feeds_google_news = {
    "EN": "https://news.google.com/rss/search?q=china+when:1d&hl=en&gl=US&ceid=US:en",
    "DE": "https://news.google.com/rss/search?q=china+when:1d&hl=de&gl=DE&ceid=DE:de",
    "FR": "https://news.google.com/rss/search?q=china+when:1d&hl=fr&gl=FR&ceid=FR:fr"
}

# === Think Tanks & Institute ===
feeds_thinktanks = {
    "MERICS": "https://merics.org/en/rss.xml",
    "CSIS": "https://www.csis.org/rss.xml",
    "CREA (Energy & Clean Air)": "https://energyandcleanair.org/feed/",
    "Brookings": "https://www.brookings.edu/feed/",
    "Peterson Institute": "https://www.piie.com/rss/all",
    "CFR ‚Äì Council on Foreign Relations": "https://www.cfr.org/rss.xml",
    "RAND Corporation": "https://www.rand.org/rss.xml",
    "Chatham House": "https://www.chathamhouse.org/rss.xml",
    "Lowy Institute": "https://www.lowyinstitute.org/the-interpreter/rss.xml"
}

# === Google News China Top-Stories ===
feeds_topchina = {
    "Google News ‚Äì China": "https://news.google.com/rss/search?q=china+when:1d&hl=en&gl=US&ceid=US:en"
}

# === SCMP & Yicai ===
feeds_scmp_yicai = {
    "SCMP": "https://www.scmp.com/rss/91/feed",
    "Yicai Global": "https://www.yicaiglobal.com/rss/news"
}

# === China-Filter & Score-Funktionen ===
def score_article(title, summary=""):
    title = title.lower()
    summary = summary.lower()
    content = f"{title} {summary}"
    must_have_in_title = [
        "china", "chinese", "xi", "beijing", "shanghai", "hong kong", "taiwan", "prc",
        "communist party", "cpc", "byd", "alibaba", "tencent", "huawei", "li qiang", "brics",
        "belt and road", "macau", "pla"
    ]
    if not any(kw in title for kw in must_have_in_title):
        return 0
    important_keywords = [
        "gdp", "exports", "imports", "tariffs", "real estate", "economy", "policy", "ai",
        "semiconductors", "pmi", "cpi", "housing", "foreign direct investment", "tech",
        "military", "sanctions", "trade", "data", "manufacturing", "industrial"
    ]
    positive_modifiers = [
        "analysis", "explainer", "comment", "feature", "official", "report", "statement"
    ]
    negative_keywords = [
        "celebrity", "gossip", "dog", "baby", "fashion", "movie", "series", "bizarre",
        "dating", "weird", "quiz", "elon musk", "rapid", "lask", "bundesliga", "eurovision",
        "basketball", "nba", "mlb", "nfl", "liberty", "yankees", "tournament", "playoffs",
        "finale", "score", "blowout"
    ]
    score = 1
    for word in important_keywords:
        if word in content:
            score += 2
    for word in positive_modifiers:
        if word in content:
            score += 1
    for word in negative_keywords:
        if word in content:
            score -= 3
    return score

# === News-Artikel filtern & bewerten ===
def fetch_news(feed_url, max_items=20, top_n=5):
    feed = feedparser.parse(feed_url)
    scored = []
    for entry in feed.entries[:max_items]:
        title = entry.get("title", "")
        summary = entry.get("summary", "")
        link = entry.get("link", "")
        score = score_article(title, summary)
        if score > 0:
            scored.append((score, f'‚Ä¢ <a href="{link.strip()}">{title.strip()}</a>'))
    scored.sort(reverse=True, key=lambda x: x[0])
    return [item[1] for item in scored[:top_n]] or ["Keine aktuellen China-Artikel gefunden."]

# === SCMP & Yicai Ranking-Wrapper ===
def fetch_ranked_articles(feed_url, max_items=20, top_n=5):
    return fetch_news(feed_url, max_items=max_items, top_n=top_n)

# === Neue Funktion: extract_source (f√ºr Google News) ===
def extract_source(title):
    """Extrahiert den Quellennamen aus dem Titel (z. B. '‚Äì Reuters')."""
    for source in source_categories:
        if f"‚Äì {source}" in title or f"- {source}" in title or title.lower().endswith(source.lower()):
            return source
    return "Unknown Source"

# === Substack aus E-Mails abrufen ===
def fetch_substack_from_email(email_user, email_password, folder="INBOX", max_results_per_sender=5):
    """Liest Substack-Mails von mehreren Absendern aus Gmail, robuste Version."""
    posts = []
    
    try:
        print(f"Debug - Aktuelles Arbeitsverzeichnis: {os.getcwd()}")
        print(f"Debug - Existiert substacks.json?: {os.path.exists('substacks.json')}")
        with open("substacks.json", "r") as f:
            substack_senders = json.load(f)
        substack_senders = sorted(substack_senders, key=lambda x: x["order"])
        # Pr√ºfe auf doppelte E-Mail-Adressen
        email_counts = defaultdict(int)
        for sender in substack_senders:
            email_counts[sender.get("email")] += 1
        duplicates = [email for email, count in email_counts.items() if count > 1 and email]
        if duplicates:
            print(f"‚ö†Ô∏è Warnung: Doppelte E-Mail-Adressen in substacks.json: {duplicates}")
    except FileNotFoundError:
        print("‚ùå Fehler: substacks.json nicht gefunden! Verwende leere Liste.")
        substack_senders = []
        posts.append(("Allgemein", "‚ùå Fehler: substacks.json nicht gefunden.", "#", "", 999))
    except json.JSONDecodeError:
        print("‚ùå Fehler: substacks.json ung√ºltig!")
        substack_senders = []
        posts.append(("Allgemein", "‚ùå Fehler: substacks.json ung√ºltig.", "#", "", 999))
    
    # Retry-Logik f√ºr Gmail-Verbindung
    for attempt in range(3):
        try:
            imap = imaplib.IMAP4_SSL("imap.gmail.com")
            imap.login(email_user, email_password)
            imap.select(folder)
            break
        except Exception as e:
            print(f"‚ùå Verbindung zu Gmail fehlgeschlagen (Versuch {attempt+1}/3): {str(e)}")
            if attempt == 2:
                return [("Allgemein", f"‚ùå Fehler beim Verbinden mit Gmail nach 3 Versuchen: {str(e)}", "#", "", 999)]
            time.sleep(2)
    
    try:
        # Datumsfilter: Letzte 3 Tage
        since_date = (datetime.now() - timedelta(days=3)).strftime("%d-%b-%Y")
        for sender in substack_senders:
            sender_email = sender.get("email")
            sender_name = sender.get("name")
            sender_order = sender.get("order", 999)  # Fallback f√ºr fehlenden order-Wert
            if not sender_email:
                posts.append((sender_name, f"‚ùå Keine E-Mail-Adresse f√ºr {sender_name} angegeben.", "#", "", sender_order))
                continue
            try:
                # Suche: Alle Mails seit since_date von Absender
                search_query = f'(FROM "{sender_email}" SINCE {since_date})'
                print(f"Debug - Suche nach: {search_query}")
                typ, data = imap.search(None, search_query)
                if typ != "OK":
                    print(f"Debug - IMAP-Suchfehler f√ºr {sender_name} ({sender_email}): {data}")
                    posts.append((sender_name, f"‚ùå Fehler beim Suchen nach Mails von {sender_name} ({sender_email}).", "#", "", sender_order))
                    continue
                email_ids = data[0].split()[-max_results_per_sender:]
                print(f"Debug - Gefundene Mail-IDs f√ºr {sender_name}: {email_ids}")
                if not email_ids:
                    posts.append((sender_name, f"üì≠ Keine Mails von {sender_name} in den letzten 3 Tagen gefunden.", "#", "", sender_order))
                    continue
                # Tempor√§re Liste f√ºr Beitr√§ge dieses Senders
                sender_posts = []
                for eid in email_ids:
                    typ, msg_data = imap.fetch(eid, "(RFC822)")
                    if typ != "OK":
                        sender_posts.append((sender_name, f"‚ùå Fehler beim Abrufen der Mail {eid} von {sender_name}.", "#", "", sender_order, None))
                        continue
                    msg = email.message_from_bytes(msg_data[0][1])
                    date_str = msg["Date"]
                    mail_date = None
                    if date_str:
                        try:
                            mail_date = parsedate_to_datetime(date_str)
                            print(f"Debug - Datum f√ºr Mail {eid} von {sender_name}: {mail_date}")
                        except (TypeError, ValueError) as e:
                            print(f"Debug - Ung√ºltiges Datum in Mail {eid} von {sender_name}: {date_str}, Fehler: {str(e)}")
                    else:
                        print(f"Debug - Kein Datum in Mail {eid} von {sender_name}")
                    html = None
                    if msg.is_multipart():
                        for part in msg.walk():
                            if part.get_content_type() == "text/html":
                                html = part.get_payload(decode=True).decode(errors="ignore")
                                break
                    elif msg.get_content_type() == "text/html":
                        html = msg.get_payload(decode=True).decode(errors="ignore")
                    if not html:
                        sender_posts.append((sender_name, f"‚ùå Kein HTML-Inhalt in der Mail {eid} von {sender_name}.", "#", "", sender_order, mail_date))
                        continue
                    soup = BeautifulSoup(html, "lxml")
                    # Erweiterte Titel-Suche
                    title_tag = (soup.find("h1") or 
                                soup.find("h2") or 
                                soup.find("h3") or 
                                soup.find("p", class_=lambda x: x and "title" in x.lower()) or
                                soup.find("div", class_=lambda x: x and "title" in x.lower()) or
                                soup.find("span", class_=lambda x: x and "title" in x.lower()))
                    if not title_tag:
                        link_tag = soup.find("a", href=lambda x: x and "/post/" in x)
                        if link_tag and link_tag.text.strip():
                            title = link_tag.text.strip()
                        else:
                            title = msg["Subject"].strip() if msg["Subject"] else "Unbenannter Beitrag"
                    else:
                        title = title_tag.text.strip()
                    print(f"Debug - Titel f√ºr {sender_name}: {title}")
                    # Link-Suche
                    link_tag = soup.find("a", href=lambda x: x and ("app-link/post" in x or "/post/" in x))
                    if not link_tag:
                        link_tag = soup.find("a", href=lambda x: x and "https://" in x)
                    link = link_tag["href"].strip() if link_tag else "#"
                    # Verbesserte Teaser-Suche
                    teaser = ""
                    if title_tag or link_tag:
                        start_tag = title_tag or link_tag
                        content_candidates = start_tag.find_all_next(string=True)
                        found_title = False
                        teaser_parts = []
                        for text in content_candidates:
                            stripped = text.strip()
                            if not found_title and stripped and (stripped in title or stripped in link):
                                found_title = True
                                continue
                            if (found_title and 30 < len(stripped) < 500 and 
                                "dear reader" not in stripped.lower() and 
                                "subscribe" not in stripped.lower() and 
                                "view in browser" not in stripped.lower()):
                                teaser_parts.append(stripped)
                                if len(" ".join(teaser_parts)) > 100:
                                    break
                        teaser = " ".join(teaser_parts).strip()[:300]
                    print(f"Debug - Teaser f√ºr {sender_name}: {teaser}")
                    sender_posts.append((sender_name, title, link, teaser, sender_order, mail_date))
                # Sortiere Beitr√§ge nach Datum (neuester zuerst)
                sender_posts.sort(key=lambda x: x[5] or datetime(1970, 1, 1), reverse=True)
                # F√ºge sortierte Beitr√§ge zu posts hinzu
                posts.extend(sender_posts)
            except Exception as e:
                posts.append((sender_name, f"‚ùå Fehler bei der Verarbeitung von {sender_name} ({sender_email}): {str(e)}", "#", "", sender_order))
        imap.logout()
    except Exception as e:
        posts.append(("Allgemein", f"‚ùå Fehler beim Verbinden mit Gmail: {str(e)}", "#", "", 999))
    return posts if posts else [("Allgemein", "Keine neuen Substack-Mails gefunden.", "#", "", 999)]

# === Substack-Posts rendern ===
def render_markdown(posts):
    """Erzeugt Markdown f√ºr Substack-Beitr√§ge, mit einer √úberschrift pro Substack."""
    if not posts:
        return ["Keine neuen Substack-Artikel gefunden."]
    
    # Gruppiere Beitr√§ge nach sender_name und speichere den order-Wert
    grouped_posts = defaultdict(list)
    sender_orders = {}
    for post in posts:
        sender_name = post[0]
        sender_order = post[4] if len(post) > 4 else 999  # sender_order ist an Position 4
        grouped_posts[sender_name].append(post)
        sender_orders[sender_name] = min(sender_orders.get(sender_name, 999), sender_order)
    
    # Sortiere Substacks nach sender_order
    sorted_senders = sorted(grouped_posts.keys(), key=lambda x: sender_orders.get(x, 999))
    
    markdown = []
    for sender_name in sorted_senders:
        markdown.append(f"### {sender_name}")
        for post in grouped_posts[sender_name]:
            if len(post) == 2:  # Fehlerfall (z. B. "üì≠ Keine Mails")
                markdown.append(f"{post[1]}\n")
            else:  # Normaler Beitrag
                title, link, teaser = post[1], post[2], post[3]
                markdown.append(f"‚Ä¢ <a href=\"{link}\">{title}</a>")
                if teaser:
                    markdown.append(f"{teaser}")
                markdown.append("")
    
    return markdown

# === NBS-Daten abrufen ===
def fetch_latest_nbs_data():
    url = "http://www.stats.gov.cn/english/PressRelease/rss.xml"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        r = requests.get(url, headers=headers, timeout=10)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        items = []
        for li in soup.select("ul.list_009 li")[:5]:
            a = li.find("a")
            if a and a.text:
                title = a.text.strip()
                link = "https://www.stats.gov.cn" + a["href"]
                items.append(f"‚Ä¢ {title} ({link})")
        return items or ["Keine aktuellen Ver√∂ffentlichungen gefunden."]
    except Exception as e:
        return [f"‚ùå Fehler beim Abrufen der NBS-Daten: {e}"]

# === B√∂rsendaten & Wechselkurse abrufen ===
def fetch_index_data():
    indices = {
        "Hang Seng Index (HSI)": "^HSI",
        "Hang Seng China Enterprises (HSCEI)": "^HSCE",
        "SSE Composite Index (Shanghai)": "000001.SS",
        "Shenzhen Component Index": "399001.SZ"
    }
    headers = {"User-Agent": "Mozilla/5.0"}
    results = []
    for name, symbol in indices.items():
        url = f"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}?interval=1d&range=2d"
        try:
            r = requests.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            data = r.json()
            closes = data["chart"]["result"][0]["indicators"]["quote"][0]["close"]
            if len(closes) < 2 or not all(closes[-2:]):
                results.append(f"‚ùå {name}: Keine g√ºltigen Kursdaten verf√ºgbar.")
                continue
            prev_close = closes[-2]
            last_close = closes[-1]
            change = last_close - prev_close
            pct = (change / prev_close) * 100
            arrow = "‚Üí" if abs(pct) < 0.01 else "‚Üë" if change > 0 else "‚Üì"
            results.append(f"‚Ä¢ {name}: {round(last_close,2)} {arrow} ({pct:+.2f}‚ÄØ%)")
        except Exception as e:
            results.append(f"‚ùå {name}: Fehler beim Abrufen ({e})")
    return results

# Interpretation f√ºr USD/CNY-Spread
def interpret_usd_cny_spread(spread_pips):
    if spread_pips <= -100:
        return "CPR stark unter Markterwartungen: starker Abwertungsdruck"
    elif -99 <= spread_pips <= -20:
        return "CPR leicht st√§rker als Markterwartungen: leichter Abwertungsdruck"
    elif -19 <= spread_pips <= 19:
        return "CPR liegt innerhalb der Markterwartungen"
    elif 20 <= spread_pips <= 99:
        return "CPR leicht schw√§cher als Markterwartungen: Markt erwartet st√§rkeren Yuan"
    else:  # >= 100
        return "CPR stark √ºber Markterwartungen: Markt dr√§ngt auf Yuan-St√§rke"

# Interpretation f√ºr CNH‚ÄìCNY-Spread
def interpret_cnh_cny_spread(spread_pips):
    if spread_pips <= -50:
        return "Starke CNY-Aufwertung"
    elif -49 <= spread_pips <= -10:
        return "Leichte CNY-St√§rke"
    elif -9 <= spread_pips <= 9:
        return "Stabile Marktbedingungen"
    elif 10 <= spread_pips <= 49:
        return "Leichte CNY-Schw√§che"
    else:  # >= 50
        return "Starke CNY-Abwertung"

def fetch_cpr_forexlive():
    urls = [
        "https://www.forexlive.com/CentralBanks",
        "https://www.forexlive.com/"
    ]
    headers = {"User-Agent": "Mozilla/5.0"}
    for url in urls:
        try:
            r = requests.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "html.parser")
            articles = soup.find_all(["h2", "h3", "div"], class_=lambda x: x and ("card__title" in x or "article" in x) if x else False)
            for article in articles:
                title = article.text.strip().lower()
                if "pboc" in title and ("usd/cny" in title or "cny" in title or "reference rate" in title or "yuan" in title):
                    match = re.search(r"\d+\.\d{4}", title)
                    if match:
                        cpr = float(match.group())
                        estimate_match = re.search(r"estimate at (\d+\.\d{4})|vs\. (\d+\.\d{4})|vs\. estimate (\d+\.\d{4})", title)
                        estimate = float(estimate_match.group(1) or estimate_match.group(2) or estimate_match.group(3)) if estimate_match else None
                        if estimate is not None:
                            pips_diff = int((cpr - estimate) * 10000)  # Pips mit Vorzeichen
                        else:
                            pips_diff = None
                        print(f"‚úÖ CPR von ForexLive gefunden: USD/CNY = {cpr}, Estimate = {estimate}, Pips = {pips_diff}")
                        return cpr, estimate, pips_diff
            print(f"‚ùå Kein CPR-Artikel auf {url} gefunden.")
            print(f"Debug - Gefundene Artikel: {[a.text.strip()[:50] for a in articles[:5]]}")
        except Exception as e:
            print(f"‚ùå Fehler beim Abrufen von ForexLive ({url}): {str(e)}")
    return None, None, None

def fetch_cpr_from_x():
    headers = {"User-Agent": "Mozilla/5.0"}
    accounts = ["ForexLive", "Sino_Market"]
    for account in accounts:
        try:
            # Scrape X search results (simplified, no API)
            search_url = f"https://x.com/search?q=from:{account}%20PBOC%20USD/CNY%20reference%20rate"
            r = requests.get(search_url, headers=headers, timeout=10)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "html.parser")
            tweets = soup.find_all("div", attrs={"data-testid": "tweetText"})
            for tweet in tweets[:3]:  # Limit to recent posts
                text = tweet.text.strip().lower()
                if "pboc" in text and ("usd/cny" in text or "cny" in text or "reference rate" in text):
                    match = re.search(r"\d+\.\d{4}", text)
                    if match:
                        cpr = float(match.group())
                        estimate_match = re.search(r"estimate at (\d+\.\d{4})|vs\. (\d+\.\d{4})|vs\. estimate (\d+\.\d{4})", text)
                        estimate = float(estimate_match.group(1) or estimate_match.group(2) or estimate_match.group(3)) if estimate_match else 7.1820  # Fallback Reuters
                        pips_diff = int((cpr - estimate) * 10000)
                        print(f"‚úÖ CPR von X (@{account}) gefunden: USD/CNY = {cpr}, Estimate = {estimate}, Pips = {pips_diff}")
                        return cpr, estimate, pips_diff
            print(f"‚ùå Kein CPR-Post von @{account} gefunden.")
        except Exception as e:
            print(f"‚ùå Fehler beim Abrufen von X (@{account}): {str(e)}")
    return None, None, None

def fetch_cpr_usdcny():
    # Lade CPR-Cache
    cpr_cache = load_cpr_cache()
    yesterday_str = (date.today() - timedelta(days=1)).isoformat()

    # Try CFETS
    url = "https://www.chinamoney.com.cn/english/bmkcpr/"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        r = requests.get(url, headers=headers, timeout=10)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        tables = soup.find_all("table")
        if not tables:
            print("‚ùå Fehler: Keine Tabellen auf der CFETS-Seite gefunden.")
            print(f"Debug - HTML-Auszug: {soup.prettify()[:500]}")
        else:
            for table in tables:
                for row in table.find_all("tr")[1:]:
                    cells = row.find_all("td")
                    if len(cells) >= 2 and cells[0].text.strip() == "USD/CNY":
                        cpr_text = cells[1].text.strip()
                        try:
                            cpr = float(cpr_text)
                            print(f"‚úÖ CPR gefunden: USD/CNY = {cpr}")
                            # Speichere CPR im Cache
                            cpr_cache[today_str] = cpr
                            save_cpr_cache(cpr_cache)
                            # Hole Vortagswert
                            prev_cpr = cpr_cache.get(yesterday_str)
                            return cpr, None, None, prev_cpr
                        except ValueError:
                            print(f"‚ùå Fehler: Ung√ºltiger CPR-Wert '{cpr_text}'")
        print("‚ùå Fehler: USD/CNY CPR nicht in den Tabellen gefunden.")
    except Exception as e:
        print(f"‚ùå Fehler beim Abrufen des CPR von CFETS: {str(e)}")

    # Try ForexLive
    print("‚ö†Ô∏è CFETS fehlgeschlagen, versuche ForexLive...")
    cpr, estimate, pips_diff = fetch_cpr_forexlive()
    if cpr is not None:
        # Speichere CPR im Cache
        cpr_cache[today_str] = cpr
        save_cpr_cache(cpr_cache)
        # Hole Vortagswert
        prev_cpr = cpr_cache.get(yesterday_str)
        return cpr, estimate, pips_diff, prev_cpr

    # Try X posts
    print("‚ö†Ô∏è ForexLive fehlgeschlagen, versuche X-Posts...")
    cpr, estimate, pips_diff = fetch_cpr_from_x()
    if cpr is not None:
        # Speichere CPR im Cache
        cpr_cache[today_str] = cpr
        save_cpr_cache(cpr_cache)
        # Hole Vortagswert
        prev_cpr = cpr_cache.get(yesterday_str)
        return cpr, estimate, pips_diff, prev_cpr

    # Final fallback: Use Reuters estimate as estimate, no CPR
    print("‚ö†Ô∏è X-Posts fehlgeschlagen, verwende Reuters-Sch√§tzung als Fallback.")
    return None, 7.1820, None, None

def fetch_currency_data():
    currencies = {
        "USDCNY": "USDCNY=X",  # Onshore
        "USDCNH": "USDCNH=X",  # Offshore
    }
    headers = {"User-Agent": "Mozilla/5.0"}
    results = {}
    
    # Hole CPR von CFETS oder ForexLive oder X
    cpr, estimate, pips_diff, prev_cpr = fetch_cpr_usdcny()
    if cpr is not None:
        results["CPR"] = (cpr, estimate, pips_diff, prev_cpr)
    else:
        results["CPR"] = ("‚ùå CPR (CNY/USD): Keine Daten verf√ºgbar.", estimate, pips_diff, prev_cpr)
    
    # Hole Onshore- und Offshore-Kurse von Yahoo Finance
    for name, symbol in currencies.items():
        url = f"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}?interval=1d&range=2d"
        try:
            r = requests.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            data = r.json()
            if not data.get("chart") or not data["chart"].get("result"):
                results[name] = f"‚ùå {name}: Keine Daten in der API-Antwort."
                continue
            result = data["chart"]["result"][0]
            closes = result["indicators"]["quote"][0]["close"]
            prev_close = result.get("meta", {}).get("chartPreviousClose")
            if not closes or len(closes) == 0 or prev_close is None:
                results[name] = f"‚ùå {name}: Keine g√ºltigen Kursdaten verf√ºgbar (closes={closes}, prev_close={prev_close})."
                continue
            last_close = closes[-1]
            if len(closes) == 1:
                change = last_close - prev_close
                pct = (change / prev_close) * 100 if prev_close != 0 else 0
            else:
                prev_close = closes[-2]
                change = last_close - prev_close
                pct = (change / prev_close) * 100 if prev_close != 0 else 0
            arrow = "‚Üí" if abs(pct) < 0.01 else "‚Üë" if change > 0 else "‚Üì"
            results[name] = (last_close, arrow, pct)
        except Exception as e:
            results[name] = f"‚ùå {name}: Unerwarteter Fehler ({str(e)})"
    return results

# === Stimmen von X ===
x_accounts = [
    {"account": "Sino_Market", "name": "CN Wire", "url": "https://x.com/Sino_Market"},
    {"account": "tonychinaupdate", "name": "China Update", "url": "https://x.com/tonychinaupdate"},
    {"account": "YuanTalks", "name": "YUAN TALKS", "url": "https://x.com/YuanTalks"},
    {"account": "Brad_Setser", "name": "Brad Setser", "url": "https://x.com/Brad_Setser"},
    {"account": "KennedyCSIS", "name": "Scott Kennedy", "url": "https://x.com/KennedyCSIS"},
]

def fetch_recent_x_posts(account, name, url):
    return [f"‚Ä¢ {name} (@{account}) ‚Üí {url}"]

# === Briefing generieren ===
def generate_briefing():
    date_str = datetime.now().strftime("%d. %B %Y")
    briefing = [f"Guten Morgen, Hado!\n\nüóìÔ∏è {date_str}\n\nüì¨ Dies ist dein t√§gliches China-Briefing.\n"]

    # B√∂rsenindizes
    briefing.append("\n## üìä B√∂rsenindizes China (08:00 Uhr MESZ)")
    if is_weekend_day or is_holiday_china:
        briefing.append("üìà Heute kein Handelstag an den chinesischen B√∂rsen.")
    else:
        briefing.extend(fetch_index_data())
    if is_weekend_day or is_holiday_hk:
        briefing.append("üìà Heute kein Handelstag an der B√∂rse Hongkong.")

    # Wechselkurse
    briefing.append("\n## üí± Wechselkurse (08:00 Uhr MESZ)")
    if is_weekend_day or is_holiday_china or is_holiday_hk:
        briefing.append("üìâ Heute keine aktuellen Wechselkurse.")
    else:
        currency_data = fetch_currency_data()
        # CPR
        cpr_data = currency_data.get("CPR")
        if isinstance(cpr_data, tuple) and isinstance(cpr_data[0], float):
            cpr, estimate, pips_diff, prev_cpr = cpr_data
            if estimate is not None:
                pips_formatted = f"Spread: CPR vs Est {pips_diff:+d} pips"
                spread_arrow = "‚Üì" if pips_diff <= -20 else "‚Üë" if pips_diff >= 20 else "‚Üí"
                usd_cny_interpretation = interpret_usd_cny_spread(pips_diff)
                if prev_cpr is not None:
                    pct_change = ((cpr - prev_cpr) / prev_cpr) * 100 if prev_cpr != 0 else 0
                    cpr_line = f"‚Ä¢ CPR (CNY/USD): {cpr:.4f} ({pct_change:+.2f}‚ÄØ%) vs. Est: {estimate:.4f} ({pips_formatted} {spread_arrow}, {usd_cny_interpretation})"
                else:
                    cpr_line = f"‚Ä¢ CPR (CNY/USD): {cpr:.4f} vs. Est: {estimate:.4f} ({pips_formatted} {spread_arrow}, {usd_cny_interpretation})"
                briefing.append(cpr_line)
            else:
                if prev_cpr is not None:
                    pct_change = ((cpr - prev_cpr) / prev_cpr) * 100 if prev_cpr != 0 else 0
                    briefing.append(f"‚Ä¢ CPR (CNY/USD): {cpr:.4f} ({pct_change:+.2f}‚ÄØ%)")
                else:
                    briefing.append(f"‚Ä¢ CPR (CNY/USD): {cpr:.4f}")
        else:
            briefing.append(str(cpr_data[0]))
            if cpr_data[1] is not None:
                briefing.append(f"  - Estimate: {cpr_data[1]:.4f}")
        # CNY/USD (Onshore)
        if isinstance(currency_data.get("USDCNY"), tuple):
            val_cny, arrow_cny, pct_cny = currency_data["USDCNY"]
            briefing.append(f"‚Ä¢ CNY/USD (Onshore): {val_cny:.4f} {arrow_cny} ({pct_cny:+.2f}‚ÄØ%)")
        else:
            briefing.append(currency_data.get("USDCNY"))
        # CNH/USD (Offshore)
        if isinstance(currency_data.get("USDCNH"), tuple):
            val_cnh, arrow_cnh, pct_cnh = currency_data["USDCNH"]
            briefing.append(f"‚Ä¢ CNH/USD (Offshore): {val_cnh:.4f} {arrow_cnh} ({pct_cnh:+.2f}‚ÄØ%)")
        else:
            briefing.append(currency_data.get("USDCNH"))
        # Spread CNH‚ÄìCNY
        if isinstance(currency_data.get("USDCNY"), tuple) and isinstance(currency_data.get("USDCNH"), tuple):
            val_cny = currency_data["USDCNY"][0]
            val_cnh = currency_data["USDCNH"][0]
            spread = val_cnh - val_cny
            spread_pips = int(spread * 10000)  # Convert to pips
            cnh_cny_interpretation = interpret_cnh_cny_spread(spread_pips)
            spread_arrow = "‚Üì" if spread_pips <= -10 else "‚Üë" if spread_pips >= 10 else "‚Üí"
            briefing.append(f"‚Ä¢ Spread CNH‚ÄìCNY: {spread:+.4f} {spread_arrow} ({cnh_cny_interpretation})")

    # Top 5 China-Stories
    briefing.append("\n## üèÜ Top 5 China-Stories laut Google News")
    for source, url in feeds_topchina.items():
        briefing.append(f"\n### {source}")
        briefing.extend(fetch_news(url, max_items=30, top_n=5))

    # NBS-Daten
    briefing.append("\n## üìà NBS ‚Äì Nationale Statistikdaten")
    briefing.extend(fetch_latest_nbs_data())

    # X-Stimmen
    briefing.append("\n## üì° Stimmen & Perspektiven von X")
    for acc in x_accounts:
        briefing.extend(fetch_recent_x_posts(acc["account"], acc["name"], acc["url"]))

    # Google News nach Sprache/Quelle
    briefing.append("\n## üåç Google News ‚Äì Nach Sprache & Quelle sortiert")
    all_articles = {
        "EN": defaultdict(list),
        "DE": defaultdict(list),
        "FR": defaultdict(list),
        "ASIA": defaultdict(list),
        "OTHER": defaultdict(list)
    }
    for lang, url in feeds_google_news.items():
        feed = feedparser.parse(url)
        for entry in feed.entries:
            title = entry.get("title", "").strip()
            summary = entry.get("summary", "").strip()
            link = entry.get("link", "").strip()
            if not title or not link:
                continue
            score = score_article(title, summary)
            if score <= 0:
                continue
            source = extract_source(title)
            category = source_categories.get(source, lang if lang in ["EN", "DE", "FR"] else "OTHER")
            if source in ["SCMP", "Nikkei Asia", "Yicai"]:
                category = "ASIA"
            clean_title = title
            if f"‚Äì {source}" in title:
                clean_title = title.split(f"‚Äì {source}")[0].strip()
            elif f"- {source}" in title:
                clean_title = title.split(f"- {source}")[0].strip()
            if clean_title.lower().endswith(source.lower()):
                clean_title = clean_title[:-(len(source))].strip("-:‚Äî‚Äì ").strip()
            all_articles[category][source].append((score, f'‚Ä¢ <a href="{link}">{clean_title}</a>'))
    category_titles = {
        "EN": "üá∫üá∏ Englischsprachige Medien",
        "DE": "üá©üá™ Deutschsprachige Medien",
        "FR": "üá´üá∑ Franz√∂sische Medien",
        "ASIA": "üåè Asiatische Medien",
        "OTHER": "üß™ Sonstige Quellen"
    }
    for cat_key, sources in all_articles.items():
        if not sources:
            continue
        briefing.append(f"\n## {category_titles.get(cat_key, cat_key)}")
        for source_name, articles in sorted(sources.items()):
            if not articles:
                continue
            briefing.append(f"\n### {source_name}")
            top_articles = sorted(articles, reverse=True)[:5]
            briefing.extend([a[1] for a in top_articles])

    # SCMP
    briefing.append("\n## üì∫ SCMP ‚Äì Top-Themen")
    briefing.extend(fetch_ranked_articles(feeds_scmp_yicai["SCMP"]))

    # Yicai
    briefing.append("\n## üìú Yicai Global ‚Äì Top-Themen")
    briefing.extend(fetch_ranked_articles(feeds_scmp_yicai["Yicai Global"]))

    # Neuer Substack-Abschnitt
    briefing.append("\n## üì¨ Aktuelle Substack-Artikel")
    substack_mail = os.getenv("SUBSTACK_MAIL")
    if not substack_mail:
        briefing.append("‚ùå Fehler: SUBSTACK_MAIL Umgebungsvariable nicht gefunden!")
    else:
        try:
            mail_pairs = substack_mail.split(";")
            mail_config = {}
            for pair in mail_pairs:
                if "=" in pair:
                    key, value = pair.split("=", 1)
                    mail_config[key] = value
            if "GMAIL_USER" not in mail_config or "GMAIL_PASS" not in mail_config:
                missing_keys = [k for k in ["GMAIL_USER", "GMAIL_PASS"] if k not in mail_config]
                briefing.append(f"‚ùå Fehler: Fehlende Schl√ºssel in SUBSTACK_MAIL: {', '.join(missing_keys)}")
            else:
                email_user = mail_config["GMAIL_USER"]
                email_password = mail_config["GMAIL_PASS"]
                posts = fetch_substack_from_email(email_user, email_password)
                briefing.extend(render_markdown(posts))
        except ValueError as e:
            briefing.append(f"‚ùå Fehler beim Parsen von SUBSTACK_MAIL: {str(e)}")

    briefing.append("\nEinen erfolgreichen Tag! üåü")

    return f"""\
<html>
  <body>
    <div style="background-color: #ffffff; padding: 20px;">
      <pre style="font-family: system-ui, sans-serif">
{chr(10).join(briefing)}
      </pre>
    </div>
  </body>
</html>"""

# === E-Mail senden ===
def send_briefing():
    print("üß† Erzeuge Briefing...")
    briefing_content = generate_briefing()

    msg = MIMEText(briefing_content, "html", "utf-8")
    msg["Subject"] = "üì∞ Dein t√§gliches China-Briefing"
    msg["From"] = config_dict["EMAIL_USER"]
    msg["To"] = config_dict["EMAIL_TO"]

    print("üì§ Sende E-Mail...")
    try:
        with smtplib.SMTP(config_dict["EMAIL_HOST"], int(config_dict["EMAIL_PORT"])) as server:
            server.starttls()
            server.login(config_dict["EMAIL_USER"], config_dict["EMAIL_PASSWORD"])
            server.send_message(msg)
        print("‚úÖ E-Mail wurde gesendet!")
    except Exception as e:
        print("‚ùå Fehler beim Senden der E-Mail:", str(e))

# === Hauptskript ===
if __name__ == "__main__":
    send_briefing()
